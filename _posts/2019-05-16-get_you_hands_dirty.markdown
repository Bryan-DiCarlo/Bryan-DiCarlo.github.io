---
layout: post
title:      "Get Your Hands Dirty!"
date:       2019-05-16 10:15:04 -0400
permalink:  get_you_hands_dirty
---



I recently finished my first, on my own merits, do it by yourself, real life data science project. The Mod 1 project.  I will not spoil the fun for those who haven’t reached that point yet with project specific details.  I can however give some insight on what I learned in the process.  It was a grind and it was exhilarating and scary all at the same time. And if you are anything like me, before beginning I had my doubts.  Where to begin, could I do it, would it be good enough.  Sure, all of that.  From almost everyone I have talked with about this project the sentiment has been the same.  Good news is you can. And perhaps the best advice I can give on the subject is just take a deep breath and start.  Yes start.  A little pre-planning is never ill advised but get started.  Go into it with a good plan but get started.  The sooner the better.  I hope to give you an idea of what I learned along the way through the common framework that is used in many data science projects.  The OSEMN framework. I will focus on obtaining and scrubbing the data.  These are generally where the most time is spent.  These are the sections where you really need to get your hands dirty.  If you spend your time wisely in these sections it will make your life much easier in the latter sections  If you are not familiar with this term it stands for Obtain the data, Scrub or clean the data, Exploratory data analysis, Modeling and evaluation and iNterpreting your results. Sure, they took some liberties with the last part of that acronym but let’s get past that for now! It gave me a great place to start.  Just to start filling some cells in my notebook.  And its also widely used in data science because that is the logical and necessary flow of any data science project.  It is also a great way to start compartmentalizing things into manageable chunks. Someone also gave me a suggestion that really helped me throughout the project.  Start each section by formulating two or three or five or however many good questions that you think need to be addressed in each section. It is a goal-oriented approach that kept me focused as I was moving forward.  Write them down.  I wrote them in my notebook.  If you won’t need them for the finalized presentation fine.  You can take them out later.  Its nice to have them there guiding you as you go along.


# Obtaining Your Data

Sounds easy enough. Right?  Well in the beginning maybe.  You might just be reading in a .csv file or something similar, creating a data frame to work with and off you go.  I am told this portion will begin to take on a whole new meaning.  Trying to gather information from different databases, in different forms all into one well organized workable format and location.  But I will leave that for another time and post.  I haven’t quite made it there yet either.  For now, let’s just says that we have read our data into a data frame and we need to begin analyzing and cleaning it.  My best advice, as it will be in other sections, is get your hands dirty. If you are using pandas look at the head of the data, run .info() and run .describe().  Run the identifier of your data frame by itself so you can get a look at the top 30 and bottom 30 rows.  Even if you do not plan to keep all of these in your final notebook.  It is a great way to see what is really in your data.  Get an idea of what if any data is missing.  Get a feel for the data type in each column.  Then start running value_counts on specific columns. Like I said even if you do not plan to keep this in your final notebook.  Value_counts will give what is in your data and how many times values occurs.  Sometimes it is hard to see this from the summary statistics alone. Its easy to see the summary statistics and think the whole column is populated by “real” values.  Then you run the value counts and realize that 70% of all values are 0.  That will make a big difference in how you approach cleaning and analyzing that information. Running value counts on columns is a great way to find place holders that might not be evident.  Sometimes a data source might not have NaN values where data is missing.  It might have something like “?” or 9999 or something similar that is totally out of place from the rest of the data.  Bottom line is it pays to really get familiar with what is in a data set you are working with.  It will give you a much better idea of what approach to take when you begin the actual cleaning of your data.  Not to mention a much clearer idea of what data will be most important when you begin the actual modeling of your data.  It can also give you an idea of what will not likely be useful in your later analysis.  All steps that will make your life easier and more productive in subsequent steps.


# Scrubbing Your Data

Now that you have a much clearer idea of what is in your data, the process of getting your data in a workable format for your analysis will be more evident.  Use the information you learned to guide you when deciding how to handle missing data.  Does the column have continuous numerical values where a proportion are just NaN or missing?  Again, go back to the value counts and verify this.  Re-visit the summary statistics for this column.  Here it might make sense to fill NaN values with mean or median.  If the column is completely skewed and the mean and median differ substantially.  Median might be the better choice.  If a column contains only 0’s and 1’s but you do a normalized value count and see that 99% of the values are 0 and only 1% of the total are missing.  It might make sense to just add zeros here.  Having a good feel for the data will make that decision easier. As will having a good feel for how that data will affect your analysis later.  To use this last example again.  If your data set is small and having a 0 where a 1 might have been will impact your model later, it may be worth the effort to impute 99% 0’s and 1% 1’s even for this small amount of missing data.  There are endless ways to address these kinds of issues.  Having a good feel for your data will be your best guide in choosing the best method. I haven’t even scratched the surface of what is involved in a massive data cleanup.  The biggest lesson I have leaned thus far is that really knowing your data pays big dividends in guiding you through this difficult portion of any data science project.


# Get Dirty and Break Some Stuff

Don’t be afraid to try things that you think you don’t know how to do.  I have a tendency to get grid locked when I want to try something out of my comfort zone.  Especially when I am working in a notebook or environment that is going to be turned in or viewed by someone else.  I mean I don’t want all my code to spontaneously break and I can never recover it right?  I mean at least that is how my brain works.  My not to ingenious hack for that is to open a “practice” notebook right along side my main one.  I load the same data frame in and then just start trying stuff.  I mean who cares if it breaks its just scratch paper.  Some of my best ideas come out that way.  Countless times I have just felt stuck.  Had an idea but was too afraid to try it in my “real” code.  Then I went over to the scratch paper notebook and just started trying things.  Sometimes it works and sometimes it doesn’t.  But most of the time I find a workable solution and ideas really begin to gel.  All because I am not afraid to break it over there on scratch paper.  Get dirty and break some stuff.  This has been a very helpful approach for me.


