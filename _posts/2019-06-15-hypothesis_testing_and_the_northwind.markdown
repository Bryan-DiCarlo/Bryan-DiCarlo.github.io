---
layout: post
title:      "Hypothesis Testing and the Northwind"
date:       2019-06-15 18:31:20 +0000
permalink:  hypothesis_testing_and_the_northwind
---

#  Northwind Insights and Hypothesis Testing

As a part of the Flatiron school data science curriculum and the module 2 project we were tasked with analyzing data from Microsoft’s fictitious Northwind Trading Company.  This fictitious company was set up by Microsoft to create a database where “real world” SQL queries could be practiced.  The goal of this project was to gather information form the Northwind database using SQL queries.  Formulate some interesting business questions to be answered from this data.  Then test these hypotheses using the appropriate hypothesis test. And in the end, hopefully, generate some useful business insight along the way.  Even if it is fictitious business insight. I did find some interesting business insights along the way.  However, the main insight I gained was a new respect and understanding for hypothesis testing.  I also learned through experience that gathering real world data, even from fake companies, is never straight forward.


# ERD’s Use Them…But Never Blindly Trust Them!
An Entity Relationship Diagram (ERD) is a snapshot of the tables and table relationships in a database. It is the map of the database.  It contains the primary keys of each table and the foreign keys through which other tables are linked.  It is your road map of how to get around a database.  It is your road map and instruction manual of how to join the table you want to get the information you need.  Indispensable right?  The answer is yes.  The problem is that in the real-world databases are updated all the time.  ERD’s are not.  ERD’s generally do a good job of giving the overall structure of the data.  But many times, table names, column names and other detail features are a little off.  Different spellings, different capitalization, CategoryID somehow becomes Id.  Things of this nature are very common. It is never a bad idea to get a list of table names from the database you will be querying. Like I said the ERD will give you a relatively good idea the tables you need to join and the keys to join them on.  Once you know the tables you want to join, print the table and column information before writing your query.  This will give you the “actual” column names, table names and spellings.  It will save you a lot of time and grief.



# Devise Some Interesting Questions
Do you remember the steps for the scientific method that you learned in 7th grade?  I was a scientist for almost 20 years, and I don’t. Google it.  It is a good way to wrap your brain around the idea of the process you should follow.  In short form it is this: make an observation, ask a question, formulate your hypothesis, make a prediction (your alternative hypothesis), test the prediction and then analyze the results and draw conclusions form your findings.  There is a definite order of operations that you should follow.  You wouldn’t want to formulate your hypothesis and then go search the database to see if you can find any data to fit your idea. Get familiar with the database and the data you will be analyzing.  From there ask some interesting questions. Then formulate the hypothesis to be tested.  I will go into more detail on that below.  This is the part where most people are still a little hazy.


# Formulate your Hypothesis
This is crucial to a good test and on the surface, this seems like a simple task. You just have to be thoughtful about your set up and what you think you can “prove”. Hypothesis testing boils down to seeing if we can find a statistical difference between two or more things.  Did sales increase from one year to the next.  Does this drug lower blood pressure compared to people not on the drug. Does this new website increase clicks compared to the previous version?  If we have the data, these are all questions we can answer with hypothesis testing. We just need to be careful how we state our hypothesis and how we present our results.  When I say these are all questions that can be “answered” with hypothesis testing, well that is a bit of a misnomer.  We can never really answer the question unless it turns out to be not true. In science, data science and life in general we can never really prove any of our questions.  We can only disprove them.  Hypothesis testing and statistics in general is rooted in probability.  It’s a game of chance.  And we are hedging our bets.  Let’s say we do find a statistically significant difference in the means between two groups we test. The test mean is higher than the control.  Let’s also say our level of significance (alpha) was set at 0.05.  This is where the p- value comes in.  If the p- value (let’s say .048) is less than our pre-set alpha level. We say that our test was statistically significant, and we reject the null hypothesis that the two means are equal and not different.  More on the null and alternative hypotheses in a minute. Does that mean that we proved that the test mean is greater than the control group?  No.  It means that there is a 95.2% chance that this difference did not occur by random chance.  It also means that there is still a 4.8% chance that these groups are not different. A 4.8% chance you rejected the null when it was true.  This is called a false positive   So, lower the alpha and that way you can be more certain or confident in your results, right? Yes and no.  It is a balancing act. On one hand lowering the alpha value will decrease the chance of false positives but at the same time it also increases the chance of false negatives. You increase the chance of not rejecting the null hypothesis when you actually should.  That is why you commonly see alpha values of 0.05.  It is a commonly accepted balance between these two types of error.  It depends on what you are looking for and what type of error you are willing to accept.


# Setting up Your Hypothesis
For a hypothesis test you need to have a null and alternative hypothesis.  I will stick to testing the means of two samples in these examples.  The null hypothesis always involves an equal sign (=, <=, >=).  It is, generally speaking, the status quo.  That the two means or groups are the same and do not differ.  In the alternative hypothesis you are usually trying to establish that a difference between the means does exist. The alternative hypothesis will be stated as less than, greater than or not equal to.  Less than when you are trying to see if the mean of the alternative group is less than the control group.  Greater than when you are trying to show the mean of the alternative group is greater than the control.  These two are called one-tailed test. Because you are only concerned with above or below the control mean.  You are only concerned with one side of the distribution.  Above or below the control mean.  Toward the right-hand tail of the distribution for greater than or towards the left-hand side of the distribution for less than. You are only concerned with one side or tail of the distribution. Setting your alternative hypothesis not equal to the control mean is a two tailed test.  You are not concerned if it is above or below the control mean (it can be either).  You just want to test if it is different from the control and it can be above or below. It is important to set up your hypothesis before you start testing.  This will determine how you test and will be important in interpreting your results.


# Choosing your Hypothesis Test
There are many tests and ways to go about hypothesis testing.  I just want to highlight some considerations that need to be addressed when doing Hypothesis testing. Most tests that you will use in Hypothesis testing have some underlying assumption requirements. Namely, that the samples are independent, they are normally distributed and have equal variance.  It is important to understand the test you are running, the assumptions of the test, and when (if at all), you can relax these assumptions.  Investigate your samples and sample distributions.  It is a very important part of the EDA in Hypothesis testing.  There are certainly statistical tests to help you address the question of normality and equal variance.  Like the Shapiro and Levene test.  These are good tests use them.  I prefer a more quantitative/qualitative mashup approach.  Its always a good idea to plot your distributions.  Histograms or sns. distplots are good ways to get a glimpse of how normally distributed your samples are. I also like probability plots from the SciPy stats module.  A good gage of equal variance is to check the ratio of the standard deviations.  If this ratio is in the range of 0.8 to 1.2 you are probably safe in most cases.  If sample size is large (> 50) you can have some flexibility in meeting these assumptions.  If your sample size is small (less than 25 or 30) you should probably adhere more strictly to these assumptions.  Its important to know the statistics behind each test.  Know when you can relax the rules and when you cannot.  Both cases exist. Data is messy and doesn’t always follow the rules for perfect normality and equal variance.  Does this mean we can’t compare the groups?  Sometimes that is exactly what it means.  If we have two samples of 10 and 15 each.  They don’t look very normal. And the ratio of the standard deviations is .0.78.  This is probably not a good candidate for hypothesis testing.  It is comparing apples to oranges.  And the samples size is small, so we don’t have much corroborating evidence that these are anything other than apples and oranges. 50 is not a magic number. Rather a good rule of thumb.  So now let’s say we have 75 and 80 in our sample groups.  We look at the distributions.  Both are skewed but look similar to one another.   The ratio of the standard deviations is 1.15.  Should we compare these two?  The answer is probably yes.  Our sample size is large enough to give confidence that any small deviations from the assumptions of the test will come out in the wash.  Because of sample size we will have enough evidence to corroborate that they are both apples. Not perfect apples.  They are not pretty apples, but they are still apples and we can do the comparison.  You never want to do a test that will give you invalid results. That is always bad.  At the same time, you don’t want to pass on a comparison that could answer some fundamental questions when it is actually valid to do the comparison.  Its not learning when you can “get away” with something.  Its working smart enough to know when comparisons are valid and when they are not.


#  Insights!

I did uncover some insight from our fake company.  But the main insight I gained from this project was a new respect and deeper understanding of hypothesis testing.  I reconfirmed that data is always messy and things are rarely exactly as they seem.  I learned to love…but never trust your ERD.  In short, I think I learned exactly what I was supposed to in this project!


The content of your blog post goes here.
